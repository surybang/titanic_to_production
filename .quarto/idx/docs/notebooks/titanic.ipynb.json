{"title":"Prediction de la survie d'un individu sur le Titanic üö¢","markdown":{"yaml":{"title":"Prediction de la survie d'un individu sur le Titanic üö¢"},"headingText":"Donn√©es","containsRefs":false,"markdown":"\n\n\n\n\n<img src=\"https://media.vogue.fr/photos/5faac06d39c5194ff9752ec9/2:3/w_1920,c_limit/076_CHL_126884.jpg\" width=\"200\">\n\n\nCe tutoriel repose sur les donn√©es et le d√©fi exemple de la communaut√© kaggle sur les donn√©es du titanic.\n\nIl s'agit √† partir de la liste des passagers du titanic et de leur survie ou non de pr√©dire la chance de survie d'un individu en fonction de son nom, age, sexe, situation familiale, √©conomique...\n\nCe notebook est inspir√© par https://www.kaggle.com/mukultiwari/titanic-top-14-with-random-forest\n\n\n\n\nOn va utiliser la librairie `pandas` pour lire les donn√©es, on y a d√©pos√© le jeu de donn√©e disponible sur Kaggle :\n\n* le `train.csv` contient les donn√©es d'apprentissage, c'est-√†-dire les variables d√©crivants des individus et s'ils ont surv√©cus ou non ;\n* le `test.csv` contient uniquement la description d'individu dont il faut pr√©dire les chances de survie. \n\nAvec la m√©thode `.info`, on pourrait v√©rifier que notre _dataset_ a :\n\n* 7 variables num√©riques : `PassengerId, Survived, Pclass, Age, SibSp, Parch, Fare`\n    * dont `PassengerId` qui est l'identifiant du passager dans le dataset\n    * dont `Survived` qui est la variable √† pr√©dire la variable _\"cible\"_\n    * dont `PClass` qui est une indication de la strate socio √©conomique\n    * dont  `SibSp` et `Parch` qui permettent de d√©terminer la situation familiale (√©poux, m√®re, fille...)\n    * `Fare` qui est un prix.\n* 5 variables non num√©riques : `Name, Sex, Ticket` un identifiant de ticket, `Cabin` un identifiant de cabin, `Embarked` le port d'embarquation (_C = Cherbourg, Q = Queenstown, S = Southampton_)\n\n## Valeurs manquantes\n\nComme g√©n√©ralement en machine learning, il va falloir traiter ces valeurs manquantes en imputant des valeurs :\n* dans le train dataset ( Age 177 manquants, Cabin 687 manquants, Embarked 2 manquants)\n* dans le test dataset ( Age 86 manquants, Cabin 327 manquants, Fare 1 manquant)\n\n## Un peu d'exploration et de feature engineering\n\nAvant d'entrainer un mod√®le, il y a g√©n√©ralement une phase exploratoire du dataset que nous allons r√©duire ici au minimum.\nIl y a aussi selon le contexte du feature engineering qui est probablement l'une des composantes les plus compliqu√©es du machine learning, il s'agit selon le contexte et le probl√®me de cr√©er des variables qui ont un sens pour contribuer √† r√©soudre notre probl√©matique.\n\nIci, on peut en faire un exemple naturellement sur le nom de l'invidu et instinctivement en extrayant le titre du nom des individus, mais cela peut etre beaucoup moins naturel selon les probl√®mes voire m√™me compl√®tement un √©tat de l'art si on prend l'exemple de la mod√©lisation des images pour le machine learning ou il s'agit, par exemple, de proposer des valeurs pond√©r√©es par sous division de partie d'image.\n\n### Statut socio√©conomique\n\nUne variable cat√©gorique qui donne une id√©e de la classe socio-√©conomique de la personne dont on donne un exemple avec seaborn pour visualiser la contribution https://seaborn.pydata.org/\n\nClairement chaque classe n'avait pas la meme chance de survie, n'est ce pas Jack?\n\n### Age\n\n## Encoder les donn√©es imput√©es ou transform√©es.\n\nA partir de cette analyse rapide, on va proposer les transformations suivantes:\n\n- `Age`: il nous faut traiter les `Null`, on impute √† la moyenne m√™me s'il y a mieux √† faire...\n- `Embarked`: il a 2 valeurs manquantes qu'on ajoute √† la valeur la plus fr√©quente _S_\n- `Fare` : m√™me sort que `Age` on impute √† la moyenne\n- `hasCabin` : Le nombre de `Null` √©tant importante on va ajouter la variable  1 ou 0 pour ne retenir que si la personne avait une cabine ou non\n\nOn _split_ notre _dataset_ d'apprentisage pour faire de la validation crois√©e une partie pour apprendre une partie pour regarder le score.\n\nPrenons arbitrairement 10% du dataset en test et 90% pour l'apprentissage.\n\n# Random Forest\n\nLes forets al√©atoires sont des algorithmes souvent performants en ce qui concerne les classifications.\n\nL'id√©e est simple :\n* On prend dans le dataset une partie des donn√©es et une partie des variables au hasard.\n* On fait un arbre de d√©cision sur ces donn√©es tir√©es al√©atoirement, l'arbre de d√©cision √©tant un algorithme permettant de d√©terminer la variable et sa valeur qui permet de s√©parer au mieux la population par rapport √† notre variable cible le but √©tant de descendre aux feuilles les plus pures.\n\n![image.png](attachment:image.png)\n\n* Puis on recommence avec une autre sous partie des donn√©es et des variables, ce qui nous fait un second arbre...\n* Du coup, plusieurs arbres al√©atoires, ca nous fait une for√™t...al√©atoire.\n* Un individu √† pr√©dire passera dans chacun des arbres et aura pour chaque arbre une pr√©diction, la pr√©diction finale √©tant la pond√©ration de chacun de nos arbres.\n\n","srcMarkdownNoYaml":"\n\n\n\n\n<img src=\"https://media.vogue.fr/photos/5faac06d39c5194ff9752ec9/2:3/w_1920,c_limit/076_CHL_126884.jpg\" width=\"200\">\n\n\nCe tutoriel repose sur les donn√©es et le d√©fi exemple de la communaut√© kaggle sur les donn√©es du titanic.\n\nIl s'agit √† partir de la liste des passagers du titanic et de leur survie ou non de pr√©dire la chance de survie d'un individu en fonction de son nom, age, sexe, situation familiale, √©conomique...\n\nCe notebook est inspir√© par https://www.kaggle.com/mukultiwari/titanic-top-14-with-random-forest\n\n\n\n## Donn√©es\n\nOn va utiliser la librairie `pandas` pour lire les donn√©es, on y a d√©pos√© le jeu de donn√©e disponible sur Kaggle :\n\n* le `train.csv` contient les donn√©es d'apprentissage, c'est-√†-dire les variables d√©crivants des individus et s'ils ont surv√©cus ou non ;\n* le `test.csv` contient uniquement la description d'individu dont il faut pr√©dire les chances de survie. \n\nAvec la m√©thode `.info`, on pourrait v√©rifier que notre _dataset_ a :\n\n* 7 variables num√©riques : `PassengerId, Survived, Pclass, Age, SibSp, Parch, Fare`\n    * dont `PassengerId` qui est l'identifiant du passager dans le dataset\n    * dont `Survived` qui est la variable √† pr√©dire la variable _\"cible\"_\n    * dont `PClass` qui est une indication de la strate socio √©conomique\n    * dont  `SibSp` et `Parch` qui permettent de d√©terminer la situation familiale (√©poux, m√®re, fille...)\n    * `Fare` qui est un prix.\n* 5 variables non num√©riques : `Name, Sex, Ticket` un identifiant de ticket, `Cabin` un identifiant de cabin, `Embarked` le port d'embarquation (_C = Cherbourg, Q = Queenstown, S = Southampton_)\n\n## Valeurs manquantes\n\nComme g√©n√©ralement en machine learning, il va falloir traiter ces valeurs manquantes en imputant des valeurs :\n* dans le train dataset ( Age 177 manquants, Cabin 687 manquants, Embarked 2 manquants)\n* dans le test dataset ( Age 86 manquants, Cabin 327 manquants, Fare 1 manquant)\n\n## Un peu d'exploration et de feature engineering\n\nAvant d'entrainer un mod√®le, il y a g√©n√©ralement une phase exploratoire du dataset que nous allons r√©duire ici au minimum.\nIl y a aussi selon le contexte du feature engineering qui est probablement l'une des composantes les plus compliqu√©es du machine learning, il s'agit selon le contexte et le probl√®me de cr√©er des variables qui ont un sens pour contribuer √† r√©soudre notre probl√©matique.\n\nIci, on peut en faire un exemple naturellement sur le nom de l'invidu et instinctivement en extrayant le titre du nom des individus, mais cela peut etre beaucoup moins naturel selon les probl√®mes voire m√™me compl√®tement un √©tat de l'art si on prend l'exemple de la mod√©lisation des images pour le machine learning ou il s'agit, par exemple, de proposer des valeurs pond√©r√©es par sous division de partie d'image.\n\n### Statut socio√©conomique\n\nUne variable cat√©gorique qui donne une id√©e de la classe socio-√©conomique de la personne dont on donne un exemple avec seaborn pour visualiser la contribution https://seaborn.pydata.org/\n\nClairement chaque classe n'avait pas la meme chance de survie, n'est ce pas Jack?\n\n### Age\n\n## Encoder les donn√©es imput√©es ou transform√©es.\n\nA partir de cette analyse rapide, on va proposer les transformations suivantes:\n\n- `Age`: il nous faut traiter les `Null`, on impute √† la moyenne m√™me s'il y a mieux √† faire...\n- `Embarked`: il a 2 valeurs manquantes qu'on ajoute √† la valeur la plus fr√©quente _S_\n- `Fare` : m√™me sort que `Age` on impute √† la moyenne\n- `hasCabin` : Le nombre de `Null` √©tant importante on va ajouter la variable  1 ou 0 pour ne retenir que si la personne avait une cabine ou non\n\nOn _split_ notre _dataset_ d'apprentisage pour faire de la validation crois√©e une partie pour apprendre une partie pour regarder le score.\n\nPrenons arbitrairement 10% du dataset en test et 90% pour l'apprentissage.\n\n# Random Forest\n\nLes forets al√©atoires sont des algorithmes souvent performants en ce qui concerne les classifications.\n\nL'id√©e est simple :\n* On prend dans le dataset une partie des donn√©es et une partie des variables au hasard.\n* On fait un arbre de d√©cision sur ces donn√©es tir√©es al√©atoirement, l'arbre de d√©cision √©tant un algorithme permettant de d√©terminer la variable et sa valeur qui permet de s√©parer au mieux la population par rapport √† notre variable cible le but √©tant de descendre aux feuilles les plus pures.\n\n![image.png](attachment:image.png)\n\n* Puis on recommence avec une autre sous partie des donn√©es et des variables, ce qui nous fait un second arbre...\n* Du coup, plusieurs arbres al√©atoires, ca nous fait une for√™t...al√©atoire.\n* Un individu √† pr√©dire passera dans chacun des arbres et aura pour chaque arbre une pr√©diction, la pr√©diction finale √©tant la pond√©ration de chacun de nos arbres.\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"titanic.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.31","theme":["cosmo","brand"],"title":"Prediction de la survie d'un individu sur le Titanic üö¢"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}